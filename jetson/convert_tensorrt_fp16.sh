#!/bin/bash
# TensorRT FP16 Conversion Script for Jetson Orin Nano
# Recommended: Best balance between speed and accuracy

echo "========================================="
echo "ğŸš€ TensorRT FP16 Conversion"
echo "========================================="

ONNX_FILE="./yolov8s_fp32.onnx"
ENGINE_FP16="./yolov8s_fp16.engine"

# Check if trtexec is available
if ! command -v /usr/src/tensorrt/bin/trtexec &> /dev/null; then
    echo "âŒ trtexec not found. Install TensorRT on Jetson first."
    exit 1
fi

echo ""
echo "ğŸ“¦ Input:  $ONNX_FILE"
echo "ğŸ“¦ Output: $ENGINE_FP16"
echo "âš™ï¸  Precision: FP16 (Half Precision)"
echo "ğŸ¯ Target: Jetson Orin Nano (40 TOPS)"
echo ""
echo "Converting... (this may take 5-10 minutes)"
echo ""

# Convert with FP16 precision
/usr/src/tensorrt/bin/trtexec \
    --onnx=$ONNX_FILE \
    --saveEngine=$ENGINE_FP16 \
    --fp16 \
    --workspace=4096 \
    --minShapes=images:1x3x800x800 \
    --optShapes=images:1x3x800x800 \
    --maxShapes=images:1x3x800x800 \
    --verbose \
    --noTF32 \
    --useSpinWait \
    --useCudaGraph \
    --separateProfileRun \
    --skipInference \
    --streams=1 \
    --avgRuns=10 \
    --duration=0

if [ $? -eq 0 ]; then
    echo ""
    echo "========================================="
    echo "âœ… FP16 TensorRT Engine Created!"
    echo "========================================="
    ls -lh $ENGINE_FP16
    echo ""
    echo "ğŸ“Š Expected Performance:"
    echo "   â€¢ Inference Speed: 30-40 FPS"
    echo "   â€¢ Latency: 25-33 ms"
    echo "   â€¢ Accuracy: ~99% of FP32"
    echo ""
    echo "ğŸ‰ Ready for deployment!"
else
    echo "âŒ FP16 conversion failed!"
    exit 1
fi
